{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 16:40:41.321165: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-04 16:40:54.826287: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from txtai.workflow import Task\n",
    "from txtai.workflow import Workflow\n",
    "from txtai.pipeline import Tabular\n",
    "from txtai.pipeline import Similarity\n",
    "from txtai.embeddings import Embeddings\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'cache'\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(x): return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9a8ac2f9c84a2f97b7b3ec7ea3e887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to rerun or Indices and Caches dont exist, run them!\n"
     ]
    }
   ],
   "source": [
    "rerun = False\n",
    "filename = \"ctgov\"\n",
    "ckptpath = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "indexfile = f'{filename}_{ckptpath.replace(\"/\", \"-\")}.index'\n",
    "snapshot_download(repo_id=ckptpath,\n",
    "                  repo_type=\"model\",\n",
    "                  cache_dir=\"cache\")\n",
    "embeddings = Embeddings({\n",
    "    \"method\": \"transformers\",\n",
    "    \"path\": ckptpath,\n",
    "    \"content\": True,\n",
    "    \"object\": True\n",
    "})\n",
    "\n",
    "if os.path.exists(indexfile) and rerun is False:\n",
    "    print(\"Indexed and Cached!\")\n",
    "    embeddings.load(indexfile)\n",
    "else:\n",
    "    print(\"Need to rerun or Indices and Caches dont exist, run them!\")\n",
    "\n",
    "    # Create tabular instance mapping input.csv fields\n",
    "    tabular = Tabular(idcolumn=\"nct_id\", content=True)\n",
    "\n",
    "    # Create workflow\n",
    "    workflow = Workflow([Task(tabular)])\n",
    "\n",
    "    # Index subset of CORD-19 data\n",
    "    data = list(workflow([f'{filename}.csv']))\n",
    "    embeddings.index(data)\n",
    "    embeddings.save(indexfile)\n",
    "    print(\"Indexing and Caching finished for the 1st time!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun = False\n",
    "filename = \"mimiciii\"\n",
    "ckptpath = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "indexfile = f'{filename}_{ckptpath.replace(\"/\", \"-\")}.index'\n",
    "snapshot_download(repo_id=ckptpath,\n",
    "                  repo_type=\"model\",\n",
    "                  cache_dir=\"cache\")\n",
    "embeddings = Embeddings({\n",
    "    \"method\": \"transformers\",\n",
    "    \"path\": ckptpath,\n",
    "    \"content\": True,\n",
    "    \"object\": True\n",
    "})\n",
    "\n",
    "if os.path.exists(indexfile) and rerun is False:\n",
    "    print(\"Indexed and Cached!\")\n",
    "    embeddings.load(indexfile)\n",
    "else:\n",
    "    print(\"Need to rerun or Indices and Caches dont exist, run them!\")\n",
    "\n",
    "    # Create tabular instance mapping input.csv fields\n",
    "    tabular = Tabular(idcolumn=\"nct_id\", content=True)\n",
    "\n",
    "    # Create workflow\n",
    "    workflow = Workflow([Task(tabular)])\n",
    "\n",
    "    # Index subset of CORD-19 data\n",
    "    data = list(workflow([f'{filename}.csv']))\n",
    "    embeddings.index(data)\n",
    "    embeddings.save(indexfile)\n",
    "    print(\"Indexing and Caching finished for the 1st time!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
